{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 1, 0, 1, 0, 1, 0, 0, 1] # Amostras esperados\n",
    "y_pred = [0, 1, 1, 1, 1, 0, 1, 0, 1, 1] # Resultado classificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Accuracy </h2>\n",
    "\n",
    "<p>Accuracy  nos diz quantos de nossos exemplos foram de fato classificados corretamente, independente da classe.</p>\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mrow><mtext>Acur</mtext><mover accent=\"true\"><mtext>a</mtext><mo>ˊ</mo></mover><mtext>cia</mtext></mrow><mo>=</mo><mfrac><mrow><mtext>N</mtext><mover accent=\"true\"><mtext>u</mtext><mo>ˊ</mo></mover><mtext>mero&nbsp;de&nbsp;previs</mtext><mover accent=\"true\"><mtext>o</mtext><mo>˜</mo></mover><mtext>es&nbsp;corretas</mtext></mrow><mrow><mtext>N</mtext><mover accent=\"true\"><mtext>u</mtext><mo>ˊ</mo></mover><mtext>mero&nbsp;total&nbsp;de&nbsp;previs</mtext><mover accent=\"true\"><mtext>o</mtext><mo>˜</mo></mover><mtext>es</mtext></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{Acurácia} = \\frac{\\text{Número de previsões corretas}}{\\text{Número total de previsões}} \n",
    "</annotation></semantics></math>\n",
    "\n",
    "<p>Uma das maiores desvantagens é que em alguns problemas a acurácia pode ser elevada mas, ainda assim, o modelo pode ter uma performance inadequada.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Precision </h2>\n",
    "\n",
    "<p>Precision também é uma das métricas mais comuns para avaliar modelos de classificação. Esta métrica é definida pela razão entre a quantidade de exemplos classificados corretamente como positivos e o total de exemplos classificados como positivos</p>\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mrow><mtext>Precis</mtext><mover accent=\"true\"><mtext>a</mtext><mo>˜</mo></mover><mtext>o</mtext></mrow><mo>=</mo><mfrac><mtext>Verdadeiros&nbsp;Positivos</mtext><mrow><mtext>Verdadeiros&nbsp;Positivos</mtext><mo>+</mo><mtext>Falsos&nbsp;Positivos</mtext></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{Precisão} = \\frac{\\text{Verdadeiros Positivos}}{\\text{Verdadeiros Positivos} + \\text{Falsos Positivos}} \n",
    "</annotation></semantics></math>\n",
    "\n",
    "<p>Em outras palavras, a precisão responde à pergunta: \"Dentre todas as classificações de classe Positivo que o modelo fez, quantas estão corretas?”</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision = precision_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Recall </h2>\n",
    "\n",
    "<p>Ao contrário de precision, recall conhecida como sensibilidade ou taxa de verdadeiro positivo (TPR), dá maior ênfase para os erros por falso negativo.</p>\n",
    "\n",
    "<p>Esta métrica é definida pela razão entre a quantidade de exemplos classificados corretamente como positivos e a quantidade de exemplos que são de fato positivos</p>\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Recall</mtext><mo>=</mo><mfrac><mtext>Verdadeiros&nbsp;Positivos</mtext><mrow><mtext>Verdadeiros&nbsp;Positivos</mtext><mo>+</mo><mtext>Falsos&nbsp;Negativos</mtext></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{Recall} = \\frac{\\text{Verdadeiros Positivos}}{\\text{Verdadeiros Positivos} + \\text{Falsos Negativos}}\n",
    "</annotation></semantics></math>\n",
    "\n",
    "<p>De todos os exemplos que são positivos, quantos foram classificados corretamente como positivos?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recall = recall_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> F1 Score </h2>\n",
    "\n",
    "<p>A métrica F1 score, leva em consideração tanto a precision quanto a recall. Ela é definida pela média harmônica entre as duas.</p>\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>F1&nbsp;Score</mtext><mo>=</mo><mn>2</mn><mo>⋅</mo><mfrac><mrow><mtext>Precision</mtext><mo>⋅</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "</annotation></semantics></math>\n",
    "\n",
    "<p>A F1 score é uma medida mais complexa e menos interpretável que o accuracy porém tendo a ser mais precisa, pois requer tanto que o recall quanto o precision sejam altos, de forma que o número de Falsos positivos e negativos diminuam.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_Score = f1_score(y_true,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
